{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fcbc4d7",
   "metadata": {},
   "source": [
    "# Resume Transformer Workflow Agent using LangGraph\n",
    "\n",
    "This notebook implements a complete resume processing workflow using LangGraph and OpenAI's GPT-4o-mini model.\n",
    "\n",
    "## Workflow Steps:\n",
    "1. **Parse**: Extract raw text from resume files (PDF, DOCX, TXT)\n",
    "2. **Extract**: Use LLM to extract structured data into JSON\n",
    "3. **Validate & Enrich**: Clean data, calculate experience, standardize skills\n",
    "4. **Store**: Insert structured data into database\n",
    "\n",
    "## Installation\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0648216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pypdf2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pytesseract\n",
      "  Using cached pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pillow\n",
      "  Using cached pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting pdf2image\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-core>=0.1 (from langgraph)\n",
      "  Downloading langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pydantic>=2.7.4 (from langgraph)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Using cached xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.12.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (3.2 kB)\n",
      "Collecting httpx>=0.25.2 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph)\n",
      "  Downloading orjson-3.11.5-cp313-cp313-macosx_15_0_arm64.whl.metadata (41 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.7.4->langgraph)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.7.4->langgraph)\n",
      "  Using cached pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.7.4->langgraph)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Using cached lxml-6.0.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in ./.venv/lib/python3.13/site-packages (from pytesseract) (25.0)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core>=0.1->langgraph)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core>=0.1->langgraph)\n",
      "  Downloading langsmith-0.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core>=0.1->langgraph)\n",
      "  Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core>=0.1->langgraph)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core>=0.1->langgraph)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.1 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting requests>=2.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph)\n",
      "  Using cached zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph)\n",
      "  Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph)\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Downloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
      "Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.3.1-py3-none-any.whl (66 kB)\n",
      "Downloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.12.0-cp313-cp313-macosx_11_0_arm64.whl (318 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Using cached pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Using cached pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading langchain_core-1.2.5-py3-none-any.whl (484 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.5.1-py3-none-any.whl (275 kB)\n",
      "Using cached pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (603 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m603.2/603.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached lxml-6.0.2-cp313-cp313-macosx_10_13_universal2.whl (8.6 MB)\n",
      "Downloading orjson-3.11.5-cp313-cp313-macosx_15_0_arm64.whl (129 kB)\n",
      "Downloading ormsgpack-1.12.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl (640 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zstandard, xxhash, uuid-utils, urllib3, typing-extensions, tqdm, tenacity, sniffio, pyyaml, pypdf2, pillow, ormsgpack, orjson, lxml, jsonpointer, jiter, idna, h11, distro, charset_normalizer, certifi, annotated-types, typing-inspection, requests, python-docx, pytesseract, pydantic-core, pdf2image, jsonpatch, httpcore, anyio, requests-toolbelt, pydantic, httpx, openai, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41/41\u001b[0m [langgraph]41\u001b[0m [langchain-core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.12.0 certifi-2025.11.12 charset_normalizer-3.4.4 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 jiter-0.12.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-1.2.5 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.1 langsmith-0.5.1 lxml-6.0.2 openai-2.14.0 orjson-3.11.5 ormsgpack-1.12.1 pdf2image-1.17.0 pillow-12.0.0 pydantic-2.12.5 pydantic-core-2.41.5 pypdf2-3.0.1 pytesseract-0.3.13 python-docx-1.2.0 pyyaml-6.0.3 requests-2.32.5 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspection-0.4.2 urllib3-2.6.2 uuid-utils-0.12.0 xxhash-3.6.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph openai pypdf2 python-docx pytesseract pillow pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75d1f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f703e76",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7fddd",
   "metadata": {},
   "source": [
    "Installing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7278e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, Optional, List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Document parsing\n",
    "import PyPDF2\n",
    "import docx\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Access the variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set up OpenAI client\n",
    "# Make sure to set your OPENAI_API_KEY environment variable\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2fcae",
   "metadata": {},
   "source": [
    "## Define State Schema\n",
    "\n",
    "The state will be passed between all agents in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71f250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ State schema defined!\n"
     ]
    }
   ],
   "source": [
    "class ResumeState(TypedDict):\n",
    "    \"\"\"State schema for the resume transformation workflow\"\"\"\n",
    "    # Input\n",
    "    file_path: str\n",
    "    file_type: str\n",
    "    \n",
    "    # Step 1: Parse\n",
    "    raw_text: Optional[str]\n",
    "    parse_error: Optional[str]\n",
    "    \n",
    "    # Step 2: Extract\n",
    "    structured_data: Optional[Dict[str, Any]]\n",
    "    extract_error: Optional[str]\n",
    "    \n",
    "    # Step 3: Validate & Enrich\n",
    "    validated_data: Optional[Dict[str, Any]]\n",
    "    validation_error: Optional[str]\n",
    "    \n",
    "    # Step 4: Store\n",
    "    database_id: Optional[int]\n",
    "    store_error: Optional[str]\n",
    "    \n",
    "    # Metadata\n",
    "    status: str\n",
    "    messages: List[str]\n",
    "\n",
    "print(\"âœ“ State schema defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a428bc",
   "metadata": {},
   "source": [
    "## Step 1: Parse Agent\n",
    "\n",
    "Extracts raw text from PDF, DOCX, or TXT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dfd1a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Parse agent defined!\n"
     ]
    }
   ],
   "source": [
    "def parse_pdf(file_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF file, with OCR fallback for image-based PDFs\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # First, try regular text extraction\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "        \n",
    "        # If no text was extracted, the PDF is likely image-based, try OCR\n",
    "        if not text.strip():\n",
    "            print(\"   âš ï¸ No text found in PDF - attempting OCR extraction...\")\n",
    "            try:\n",
    "                from pdf2image import convert_from_path\n",
    "                import pytesseract\n",
    "                \n",
    "                # Convert PDF pages to images\n",
    "                images = convert_from_path(file_path)\n",
    "                \n",
    "                # Perform OCR on each page\n",
    "                for i, image in enumerate(images):\n",
    "                    page_text = pytesseract.image_to_string(image)\n",
    "                    text += page_text + \"\\n\"\n",
    "                    print(f\"   âœ“ OCR extracted {len(page_text)} characters from page {i+1}\")\n",
    "                \n",
    "            except ImportError:\n",
    "                raise Exception(\n",
    "                    \"PDF appears to be image-based but OCR libraries not available. \"\n",
    "                    \"Install with: pip install pytesseract pdf2image pillow\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"OCR extraction failed: {str(e)}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error parsing PDF: {str(e)}\")\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def parse_docx(file_path: str) -> str:\n",
    "    \"\"\"Extract text from DOCX file\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "        text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error parsing DOCX: {str(e)}\")\n",
    "    return text.strip()\n",
    "\n",
    "def parse_txt(file_path: str) -> str:\n",
    "    \"\"\"Extract text from TXT file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error parsing TXT: {str(e)}\")\n",
    "    return text.strip()\n",
    "\n",
    "def parse_agent(state: ResumeState) -> ResumeState:\n",
    "    \"\"\"\n",
    "    Agent 1: Parse - Extract raw text from resume file\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” PARSE AGENT: Processing {state['file_path']}\")\n",
    "    \n",
    "    try:\n",
    "        file_type = state['file_type'].lower()\n",
    "        \n",
    "        if file_type == 'pdf':\n",
    "            raw_text = parse_pdf(state['file_path'])\n",
    "        elif file_type == 'docx':\n",
    "            raw_text = parse_docx(state['file_path'])\n",
    "        elif file_type == 'txt':\n",
    "            raw_text = parse_txt(state['file_path'])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "        \n",
    "        state['raw_text'] = raw_text\n",
    "        state['parse_error'] = None\n",
    "        state['messages'].append(f\"âœ“ Successfully parsed {len(raw_text)} characters from {file_type.upper()}\")\n",
    "        print(f\"   âœ“ Extracted {len(raw_text)} characters\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['parse_error'] = str(e)\n",
    "        state['messages'].append(f\"âœ— Parse error: {str(e)}\")\n",
    "        print(f\"   âœ— Error: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ“ Parse agent defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dad187",
   "metadata": {},
   "source": [
    "## Step 2: Extract Agent\n",
    "\n",
    "Uses GPT-4o-mini to extract structured data from the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c0da7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Extract agent defined!\n"
     ]
    }
   ],
   "source": [
    "def extract_agent(state: ResumeState) -> ResumeState:\n",
    "    \"\"\"\n",
    "    Agent 2: Extract - Use LLM to extract structured data from raw text\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ¤– EXTRACT AGENT: Analyzing resume with GPT-4o-mini\")\n",
    "    \n",
    "    if state.get('parse_error'):\n",
    "        state['extract_error'] = \"Cannot extract: parsing failed\"\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        # Prepare the prompt for GPT-4o-mini\n",
    "        system_prompt = \"\"\"You are an expert resume parsing engine. Your task is to extract structured, normalized data from raw resume text and return it as VALID JSON ONLY.\n",
    "\n",
    "        Follow these rules strictly:\n",
    "        - Output MUST be a single JSON object and nothing else (no markdown, no comments).\n",
    "        - If a field is missing or not found, use null or an empty array (never invent data).\n",
    "        - Normalize capitalization (Title Case for names, companies, titles).\n",
    "        - Deduplicate repeated skills, certifications, and languages.\n",
    "        - Do not infer dates or employers unless explicitly stated.\n",
    "        - Keep descriptions concise and factual.\n",
    "\n",
    "        Extract the following schema exactly:\n",
    "\n",
    "        {\n",
    "        \"contact\": {\n",
    "            \"name\": string | null,\n",
    "            \"email\": string | null,\n",
    "            \"phone\": string | null,\n",
    "            \"location\": string | null,\n",
    "            \"linkedin\": string | null,\n",
    "            \"github\": string | null\n",
    "        },\n",
    "        \"summary\": string | null,\n",
    "        \"experience\": [\n",
    "            {\n",
    "            \"company\": string | null,\n",
    "            \"title\": string | null,\n",
    "            \"start_date\": string | null,\n",
    "            \"end_date\": string | null,\n",
    "            \"description\": string | null,\n",
    "            \"responsibilities\": [string]\n",
    "            }\n",
    "        ],\n",
    "        \"education\": [\n",
    "            {\n",
    "            \"institution\": string | null,\n",
    "            \"degree\": string | null,\n",
    "            \"field\": string | null,\n",
    "            \"graduation_year\": string | null\n",
    "            }\n",
    "        ],\n",
    "        \"skills\": [string],\n",
    "        \"certifications\": [string],\n",
    "        \"languages\": [string]\n",
    "        }\n",
    "\n",
    "        Date formatting rules:\n",
    "        - Use \"YYYY-MM\" when month is available.\n",
    "        - Use \"YYYY\" if only the year is available.\n",
    "        - Use \"Present\" for ongoing roles.\n",
    "\n",
    "        Responsibilities should be concise bullet-style statements (max 1 sentence each).\"\"\"\n",
    "\n",
    "        user_prompt = f\"Resume text:\\n\\n{state['raw_text']}\"\n",
    "        \n",
    "        # Call OpenAI API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        # Parse the response\n",
    "        extracted_text = response.choices[0].message.content\n",
    "        structured_data = json.loads(extracted_text)\n",
    "        \n",
    "        state['structured_data'] = structured_data\n",
    "        state['extract_error'] = None\n",
    "        state['messages'].append(\"âœ“ Successfully extracted structured data using GPT-4o-mini\")\n",
    "        print(f\"   âœ“ Extracted data for: {structured_data.get('contact', {}).get('name', 'Unknown')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['extract_error'] = str(e)\n",
    "        state['messages'].append(f\"âœ— Extract error: {str(e)}\")\n",
    "        print(f\"   âœ— Error: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ“ Extract agent defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db30314",
   "metadata": {},
   "source": [
    "## Step 3: Validate & Enrich Agent\n",
    "\n",
    "Cleans data, calculates total years of experience, and standardizes skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec0430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Validate & Enrich agent defined!\n"
     ]
    }
   ],
   "source": [
    "# Standard skills taxonomy for normalization\n",
    "STANDARD_SKILLS = {\n",
    "    # Programming Languages\n",
    "    'python': 'Python', 'java': 'Java', 'javascript': 'JavaScript', 'js': 'JavaScript',\n",
    "    'typescript': 'TypeScript', 'c++': 'C++', 'c#': 'C#', 'ruby': 'Ruby',\n",
    "    'go': 'Go', 'rust': 'Rust', 'swift': 'Swift', 'kotlin': 'Kotlin', 'php': 'PHP',\n",
    "    \n",
    "    # Frameworks\n",
    "    'react': 'React', 'reactjs': 'React', 'angular': 'Angular', 'vue': 'Vue.js',\n",
    "    'django': 'Django', 'flask': 'Flask', 'spring': 'Spring', 'nodejs': 'Node.js',\n",
    "    'express': 'Express.js', 'fastapi': 'FastAPI',\n",
    "    \n",
    "    # Databases\n",
    "    'sql': 'SQL', 'mysql': 'MySQL', 'postgresql': 'PostgreSQL', 'mongodb': 'MongoDB',\n",
    "    'redis': 'Redis', 'oracle': 'Oracle', 'dynamodb': 'DynamoDB',\n",
    "    \n",
    "    # Cloud & DevOps\n",
    "    'aws': 'AWS', 'azure': 'Azure', 'gcp': 'Google Cloud', 'docker': 'Docker',\n",
    "    'kubernetes': 'Kubernetes', 'k8s': 'Kubernetes', 'terraform': 'Terraform',\n",
    "    'jenkins': 'Jenkins', 'ci/cd': 'CI/CD', 'git': 'Git',\n",
    "    \n",
    "    # Data Science & ML\n",
    "    'machine learning': 'Machine Learning', 'ml': 'Machine Learning',\n",
    "    'deep learning': 'Deep Learning', 'tensorflow': 'TensorFlow',\n",
    "    'pytorch': 'PyTorch', 'pandas': 'Pandas', 'numpy': 'NumPy',\n",
    "    'scikit-learn': 'Scikit-learn', 'nlp': 'NLP',\n",
    "    \n",
    "    # Other\n",
    "    'agile': 'Agile', 'scrum': 'Scrum', 'rest api': 'REST API',\n",
    "    'graphql': 'GraphQL', 'microservices': 'Microservices'\n",
    "}\n",
    "\n",
    "def parse_date(date_str: str) -> Optional[datetime]:\n",
    "    \"\"\"Parse various date formats\"\"\"\n",
    "    if not date_str or date_str.lower() in ['present', 'current', 'now']:\n",
    "        return datetime.now()\n",
    "    \n",
    "    # Try different formats\n",
    "    formats = ['%Y-%m', '%Y', '%m/%Y', '%B %Y', '%b %Y']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(str(date_str).strip(), fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def calculate_experience(experience_list: List[Dict]) -> float:\n",
    "    \"\"\"Calculate total years of experience\"\"\"\n",
    "    total_months = 0\n",
    "    \n",
    "    for job in experience_list:\n",
    "        start = parse_date(job.get('start_date', ''))\n",
    "        end = parse_date(job.get('end_date', ''))\n",
    "        \n",
    "        if start and end:\n",
    "            months = (end.year - start.year) * 12 + (end.month - start.month)\n",
    "            total_months += max(0, months)\n",
    "    \n",
    "    return round(total_months / 12, 1)\n",
    "\n",
    "def standardize_skills(skills: List[str]) -> List[str]:\n",
    "    \"\"\"Normalize skills to standard taxonomy\"\"\"\n",
    "    standardized = set()\n",
    "    \n",
    "    for skill in skills:\n",
    "        skill_lower = skill.lower().strip()\n",
    "        # Check if skill matches standard taxonomy\n",
    "        if skill_lower in STANDARD_SKILLS:\n",
    "            standardized.add(STANDARD_SKILLS[skill_lower])\n",
    "        else:\n",
    "            # Keep original if not in taxonomy\n",
    "            standardized.add(skill.strip())\n",
    "    \n",
    "    return sorted(list(standardized))\n",
    "\n",
    "def clean_contact_info(contact: Dict) -> Dict:\n",
    "    \"\"\"Clean and validate contact information\"\"\"\n",
    "    cleaned = {}\n",
    "    \n",
    "    # Clean email\n",
    "    if contact.get('email'):\n",
    "        email = contact['email'].strip().lower()\n",
    "        if '@' in email:\n",
    "            cleaned['email'] = email\n",
    "    \n",
    "    # Clean phone\n",
    "    if contact.get('phone'):\n",
    "        phone = re.sub(r'[^\\d+\\-() ]', '', contact['phone'])\n",
    "        cleaned['phone'] = phone\n",
    "    \n",
    "    # Copy other fields\n",
    "    for field in ['name', 'location', 'linkedin', 'github']:\n",
    "        if contact.get(field):\n",
    "            cleaned[field] = contact[field].strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def validate_and_enrich_agent(state: ResumeState) -> ResumeState:\n",
    "    \"\"\"\n",
    "    Agent 3: Validate & Enrich - Clean data, calculate experience, standardize skills\n",
    "    \"\"\"\n",
    "    print(f\"âœ¨ VALIDATE & ENRICH AGENT: Processing data\")\n",
    "    \n",
    "    if state.get('extract_error'):\n",
    "        state['validation_error'] = \"Cannot validate: extraction failed\"\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        data = state['structured_data'].copy()\n",
    "        \n",
    "        # Clean contact info\n",
    "        if data.get('contact'):\n",
    "            data['contact'] = clean_contact_info(data['contact'])\n",
    "        \n",
    "        # Calculate total experience\n",
    "        if data.get('experience'):\n",
    "            total_experience = calculate_experience(data['experience'])\n",
    "            data['total_years_experience'] = total_experience\n",
    "            print(f\"   âœ“ Calculated {total_experience} years of experience\")\n",
    "        \n",
    "        # Standardize skills\n",
    "        if data.get('skills'):\n",
    "            original_count = len(data['skills'])\n",
    "            data['skills'] = standardize_skills(data['skills'])\n",
    "            print(f\"   âœ“ Standardized {original_count} skills to {len(data['skills'])} unique skills\")\n",
    "        \n",
    "        # Add metadata\n",
    "        data['processed_date'] = datetime.now().isoformat()\n",
    "        data['data_version'] = '1.0'\n",
    "        \n",
    "        state['validated_data'] = data\n",
    "        state['validation_error'] = None\n",
    "        state['messages'].append(\"âœ“ Successfully validated and enriched data\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['validation_error'] = str(e)\n",
    "        state['messages'].append(f\"âœ— Validation error: {str(e)}\")\n",
    "        print(f\"   âœ— Error: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ“ Validate & Enrich agent defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebaa60",
   "metadata": {},
   "source": [
    "## Step 4: Store Agent\n",
    "\n",
    "Formats data into SQL and inserts into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605e2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Store agent defined!\n"
     ]
    }
   ],
   "source": [
    "def initialize_database(db_path: str = \"resume_ats.db\"):\n",
    "    \"\"\"Initialize the ATS database with required tables\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create candidates table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS candidates (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT NOT NULL,\n",
    "            email TEXT,\n",
    "            phone TEXT,\n",
    "            location TEXT,\n",
    "            linkedin TEXT,\n",
    "            github TEXT,\n",
    "            summary TEXT,\n",
    "            total_years_experience REAL,\n",
    "            processed_date TEXT,\n",
    "            data_version TEXT,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create experience table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS experience (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            candidate_id INTEGER,\n",
    "            company TEXT,\n",
    "            title TEXT,\n",
    "            start_date TEXT,\n",
    "            end_date TEXT,\n",
    "            description TEXT,\n",
    "            FOREIGN KEY (candidate_id) REFERENCES candidates (id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create education table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS education (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            candidate_id INTEGER,\n",
    "            institution TEXT,\n",
    "            degree TEXT,\n",
    "            field TEXT,\n",
    "            graduation_year TEXT,\n",
    "            FOREIGN KEY (candidate_id) REFERENCES candidates (id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create skills table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS skills (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            candidate_id INTEGER,\n",
    "            skill TEXT,\n",
    "            FOREIGN KEY (candidate_id) REFERENCES candidates (id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create certifications table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS certifications (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            candidate_id INTEGER,\n",
    "            certification TEXT,\n",
    "            FOREIGN KEY (candidate_id) REFERENCES candidates (id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def store_agent(state: ResumeState, db_path: str = \"resume_ats.db\") -> ResumeState:\n",
    "    \"\"\"\n",
    "    Agent 4: Store - Insert structured data into database\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ’¾ STORE AGENT: Saving to database\")\n",
    "    \n",
    "    if state.get('validation_error'):\n",
    "        state['store_error'] = \"Cannot store: validation failed\"\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        # Initialize database\n",
    "        initialize_database(db_path)\n",
    "        \n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        data = state['validated_data']\n",
    "        contact = data.get('contact', {})\n",
    "        \n",
    "        # Insert candidate\n",
    "        cursor.execute('''\n",
    "            INSERT INTO candidates \n",
    "            (name, email, phone, location, linkedin, github, summary, \n",
    "             total_years_experience, processed_date, data_version)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            contact.get('name'),\n",
    "            contact.get('email'),\n",
    "            contact.get('phone'),\n",
    "            contact.get('location'),\n",
    "            contact.get('linkedin'),\n",
    "            contact.get('github'),\n",
    "            data.get('summary'),\n",
    "            data.get('total_years_experience'),\n",
    "            data.get('processed_date'),\n",
    "            data.get('data_version')\n",
    "        ))\n",
    "        \n",
    "        candidate_id = cursor.lastrowid\n",
    "        \n",
    "        # Insert experience\n",
    "        for exp in data.get('experience', []):\n",
    "            cursor.execute('''\n",
    "                INSERT INTO experience \n",
    "                (candidate_id, company, title, start_date, end_date, description)\n",
    "                VALUES (?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                candidate_id,\n",
    "                exp.get('company'),\n",
    "                exp.get('title'),\n",
    "                exp.get('start_date'),\n",
    "                exp.get('end_date'),\n",
    "                exp.get('description')\n",
    "            ))\n",
    "        \n",
    "        # Insert education\n",
    "        for edu in data.get('education', []):\n",
    "            cursor.execute('''\n",
    "                INSERT INTO education \n",
    "                (candidate_id, institution, degree, field, graduation_year)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                candidate_id,\n",
    "                edu.get('institution'),\n",
    "                edu.get('degree'),\n",
    "                edu.get('field'),\n",
    "                edu.get('graduation_year')\n",
    "            ))\n",
    "        \n",
    "        # Insert skills\n",
    "        for skill in data.get('skills', []):\n",
    "            cursor.execute('''\n",
    "                INSERT INTO skills (candidate_id, skill)\n",
    "                VALUES (?, ?)\n",
    "            ''', (candidate_id, skill))\n",
    "        \n",
    "        # Insert certifications\n",
    "        for cert in data.get('certifications', []):\n",
    "            cursor.execute('''\n",
    "                INSERT INTO certifications (candidate_id, certification)\n",
    "                VALUES (?, ?)\n",
    "            ''', (candidate_id, cert))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        state['database_id'] = candidate_id\n",
    "        state['store_error'] = None\n",
    "        state['status'] = 'completed'\n",
    "        state['messages'].append(f\"âœ“ Successfully stored candidate with ID {candidate_id}\")\n",
    "        print(f\"   âœ“ Stored candidate with ID: {candidate_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['store_error'] = str(e)\n",
    "        state['status'] = 'failed'\n",
    "        state['messages'].append(f\"âœ— Store error: {str(e)}\")\n",
    "        print(f\"   âœ— Error: {str(e)}\")\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ“ Store agent defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436d21f",
   "metadata": {},
   "source": [
    "## Build LangGraph Workflow\n",
    "\n",
    "Now let's create the LangGraph state machine that orchestrates all agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "238f0340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LangGraph workflow created successfully!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJoAAAITCAIAAABwvZJ5AAAQAElEQVR4nOydB1wTZx/Hn7vLIOypshG3IqJitWrd1rpXrVuxtUpbrXtb91616qvWat3WXbV11N3WVSeKdaKACxAB2SEhd+8/OYwBAxpL7sKT5yufePc8zz25y++e9X+WhOM4RMAFCSJgBJETK4icWEHkxAoiJ1YQObHCUuSMOJ38+F5WVrpGk4vUqoJtJ4pGiEOGTSqK0n7muVBaX4CmKZbl9AG4PEfEsq8v5MNQNMWxXAFHWufI5b/c4B7yXcLAKcXa2Emc3CQVQ+0DKjsgC4ASt915eGPc06jsnCyWYZBUTknlNM3QrLpgMJBTe5uGd0ppz6j8cmqDsa8DvHLkOJbKFxWbPyQ4MojTaB01ICqiDC8vcKHBJRyrYTUaSpkF/2mD27tIajZzDm7gjMRDNDn3r37y7KFSJqN8Ktk16OBq7yRDJZm7V9IiTr9MilNJZFSDjq7V6rogMRBBzoTHmfv+FyeVM426upYPdkJ4cXRrfNS1DEc3SZ8JAUhwhJbz1M74W/9k1Grh9GFrD4QvW+fFpr5Qf72oPBIWQeWMvZ1+aH3CVwuEfkhROHMg7vpfmd8Iq6hwch7dGhcdmTl4nlVoyXPzfNKfu1O+WSzcI9NIEP69kBIVYV1aAkEfutVs6rhmwgMkFALJeXpXUqu+pZH1Ub9dKTsnyS8LYpEgCCHn5jkxLqUk5YItoqEtPL3H+ycnqO9dTUPmx+xyJj/PTk3M7TUuAFkxgdVt/9qbiMyP2eU8vC7BycPaLcOtw7xysrjYWxnIzJhdzpTE3AYd3JDV41xKcva3JGRmzCvnpWNJYAEPDBK01Hzw4EG7du2Q6ezcuXPq1KnIPFSp6wiGBWRmzCvnwxsZ9k4MEpZbt26h9+K9L3wXajV1hY6dF/HZyJyYV87MVI1zKXPZ1tPT0xcuXNixY8ePPvpo8ODB+/btA8fVq1dPnz49Pj4+NDR069at4PL3339Pnjy5bdu2DRs2DA8Pv3z5Mn/59u3bW7Vqdfr06Q8++GDRokWDBg36/fffDx48CBfeuXMHmQGJlLpz0bzFp3krKdBz6VrGXHKCbAkJCRMmTChbtizkk3Pnzg0MDATBVCrV0aNHQRsIo1QqQUsQDALD6fHjx0eMGAHCu7m5yWSyzMzM3bt3z5gxo2rVqn5+fmFhYf7+/nxIcwDdfykJKmROzCsn9PfaOZkrA7h69Wq/fv3q1asHx0OHDm3RooWzc8G+RhsbG0iFCoWC9woKCgL9IiIimjdvTlEUiN2/f/86deogQWAYSp1tXpOqeeWEn4xD5io7Q0JCtmzZ8vLly1q1an344YdVqlQxGgyS4IoVK65cufLixQveJSUlRe9brVo1JBRgHmfNbCE3c0OF4pSZ5qrOTZs2rVevXufPnx85cmTLli1XrVqVm5tbIAwUogMHDlSr1XPmzIGQFy5cKBAAslwkFJpcTiqjkDkxb+pkGDolzlylhaOj4+effz5gwIDr16+fOnVq3bp1Dg4Offr0MQxz7NgxKEqhOIT8FuVPl8KjzmEd3aXInJhXToUjA+ZKZAZSU1OPHDkC1VooHUN03L17980aKQQD1XktgRMnTiDxyFWjCiH2yJyYN7MNqKrIytAgMyCRSNasWTNu3DhImklJSdDAAC1BVPCCOioUk9ACiY2NrVChAhzv2bMH8uFz585dvHgR6kSQAxuN09fX9+bNm5cuXUpOTkbFza0L2ozBp4IdMicMlEDIbPhVsrtwMLlCLXuFXTFXiKDMq169OuSl69evhwrR48ePv/zyy06dOkHly93dHQwCGzZsAOW6d++u0Wi2bdu2bNkyyGknTZqUlZW1efNm0NjDwwOapFCy0nTeO+3i4gIuv/zyS926dX18fFCx8sfmeA5xtZu5InNi9tEI66ZE2zsy3Uf7Ietmxciohh3cQpqYd4Sf2U3wjbu6JT41b9vZ8jm167lEQplbSyTAKPjyNRxlihd7lj3u+q2v0QD79+///vvvjXrl5OTI5XKjXlBGNGnSBJmHImKGMhiKbaNekIdD6WvU699zabVaOCLzI9DQrxUjosIXBhj9IaAhAbIZvQqsNlBxNeoFldXCftb/DliDC/MqQk47Ozt9MWzInhWPXiaov5hZDpkfgeQ88UtCVETG4PlCPJJFEXsr/fd1CYIN5hNo6FfznqWh/3bDjGhkZYCWnw7zQkIh6LDps78lQikyaK5VpNGXiaqtcx/1n+pv72ReS5AhQk9q2PX9I+gkgnaLk3vJnmNUNH9sfHY/Iqv3eF+X0nIkICJMOTqzP/H6n6kevrLPRmDYGL1/Ne3U7kTEcoPEGCMu2oTATbOi01M0zqWktZs5Va4j5pzI4uLkjoSHkRkqJVc2yLZ1mHDlpSFiTtdNjs85vDEu9UUu4pCNLW3nzCjsJVIb2nB2LdJ2msI9vp5CW2BedJ4v75V/ii1NIX66tKE7wyCNJp/j6wMq79fQzrPm5wPnueu+Eb4EuivhRHfO0NDE0uRkajJS1ZlpHKdBEjkYNW3bDBBHyLxbtYRVv+5cfnn/aubLRLVaxeaqOE2BXsv8P3wBzfQa5A9a8IxlWb5RqJ9wb/Ba5IV7HVWBaPQhdaLSICviaEYbXCKlbOyo0gGKep+42lnAjGOLkNPcgEV+7ty5YLVBuGMV49OLMOVgBpETK4icWGEVD6lWq6VS4UwzIkJSJ1YQObGCyIkVRE6sIHJiBZETK4icWEHkxApiRsAKkjqxgqROrBBoYKa4kNSJFaTsxAqSOrGCyIkVRE6sIHJiBZETK4icWGFvby/kclAiYhVyZmVlKZVKZAVYRxYkkby5vhuWEDmxwirkZBhGozHL4mOWBkmdWEHkxAoiJ1YQObGCyIkVRE6ssIrBJSR1YgWREyuInFhB5MQKIidWEJstVlhP6sR51a/OnTvHxMRQ1OsV/uDY1dX12LFjCFNwbneGh4c7ODjQBrAsW7duXYQvOMvZqlWrsmXLGrp4enr27NkT4QvmVqEBAwY4OTnpTytXrizkDo/Cg7mcTZo0qVChAn/s6OjYt29fhDX422zDwsJASDioUqVKzZo1EdZYSs028lxSfIxardvdiqY5VrfgNENzGt2BRIL4hgZDIw2L9F5QaaXpvNWj9WtL5x2j10tTR0REvHz5Mjg4yNXVPS/Aq69ASLestHZh4lfX0ojVfQV/LUNTGvb1TyRhUK5hC5ZCCgWq19JJ4aJAFoD4cibHq3YvewRCyuW0Kkd7M4yE1uSyugPErzwtlVFqlW5Zb4ZiNbowDKXR6Fbyhl+fXwxcu+T3q+Wjaa2W8GR6beAxIVpWd4lWJIbW8O+FTnuQjn117evVxXUR0kxe/Hm+r24p71oGoqVUSs7Rne47IRCJjchyJj9XbV/4qFp9p1rNPFBJZtfSKBuFpNfYACQqIsu5cnRUu3BvFw+LyKn+I/tXxcCP2XdCWSQeYlaFdi9/pHCk8dAS6PhVQGqiJiMlG4mHmHKmJua6e9ogjJDZoEvH05B4iGmCV+ewjBSrPgCOpbLSxSy8xPw1ocbI5WLVb8VqoDLMIvGwig4y60FMOSldiw8Rig9R5dRZbXACzBe6d1Q0xJST039ghbVWhTgOt6EQHKv9ExEx5aQRhVlmKzqipk48tbTashO7zFaHtZadhGJH1LKTJmVnMSOqkY97tak1oZgQNbPlEGbtTu0oCFHfT1J2FifaRqeo76eoRj5tW4VktsWJqKmTgszJhJf53v07g8P7TJ+2YOOmNQ8fRrm5uTdt8vE3X4/kfc+f//vkqT9uRF5LS0utUjmob9+BNUNCwX3P3u3bflk/YviEqdPGdur02dBvRj96FLN+w+qI61eg6K5WLbjHZ/2qVw9BunVS1/288sI/Z54/jw8KCunc8bN69RoiU9DabEV9P8UcjWBqu1PCaF++LVvWzZq55I/D5775etT+A7sOHtoHjkqlcvbcyTk5OePHTZ8ze6mfX8CkySOSk5PASyaTZWVlHjiwe8L4GaCQSqUaPnIQwzDz5y1fvHAVxAkh+fU0ly1fsHvPts6dum/b+lvjRs2nTh/7518nkClojXxWndmaXnP46KNmnmW84KBpk5bHTxw+ceJI2zadbGxs1q7ZrlAonJycwQtS5/4DuyNvRoAq0BgCtXr06F+rZh3wevDgfkpKctcuPStWqAynU6fMu37jKqRLeBX+OPp7r55hHdp3Bfc2rTvevHl90+afIAYTbk7sokPkHhXK9JpDhfKV9MfeXr6gKH8MSXDtuhWQhSYlveBdXr5M0YesXClvaoqPj5+zs8u8BdNatmgTUqN2UFANPk+OjIyAhFsn9EP9JeB7+MiBjIwMe3t79O6PJCqipk7tqGeT32cbG4XBsU1mZgYcJCTEDxsxsFbND76bNKdq1eoQc8tW9Qyv0q82LZfLf/j+J8iiIV+FktLLyyes36CWLdtkZKSD79BhXxT4urT0VBPkFBux252mv878784DuSiv7uk/j0HagoIT8luUP12+CZSsX4UPHxAWfvXqRUh/c+ZN8Q8IdHPXjtseNXKSt7evYWBXFzf0zlh19zV6n8SJIDtt2LAJfxwVdTewbHk4gNqsg4MjryVQRBUGqrX/3rrR+pMOkLLr129Ut26DT9o0uHfvdrOmrSDhQgA+7wWgiIWqGgRD74yuv1PMDFfkGWTv8eiXLp//5+I5ODhz9vS1iMstWrSG48DAClBkHvhtD1RqwBeSHdSJoL3x5uUg/IKFM1atXvrk6ePHj2O3blsPlwRVq2FraxvWfzDUffhCFF6I0WO/XvrDPFSiELmD7D3q9b16hK1b97/xE76labpLlx5QrQXH5s1axcY+BDG+Xzq3Tmi9cWOnbd+xadsvG9LT0ypWrGJ4OdR9Ro6YuGHjjzt3bYHT0Np1lyxeHRCgnS3Uo3u/cuUqbtu+Ad4GOzv7alWDR42ajEoUYs5R+XHcA9/K9h91Kf2O4cF08MWXPaAiExxsodM0t8x66FvJpt1ALyQSYqZOjYbjNKIOrSluKLHHy4gpJwMdnjRW0791ViFrHY2gYVmTpgAEBpY/deIysmSs2SqkHYyAWY+KNVuFdM9OOsiKE5FN8NqJDYTiQ+yxQiyRszgRtexkKG0HNkZon0bUBxLVZgvVerwyW+1YIaudowKZLcIrs7XqHhX8Giqi96iIbYInFCsitzuJosWLmHLK5QyN17BtmRxJbcSs2opqgpdzGalqhBE5Oay7p7gzoMWjfA2H5Hh85HwQkQoVu9rN3ZF4iClnw44ecjnavfQBwoLzBxODGzshURF/Pdu9Kx4lPlP5VbT1LGcrkUjze3IFbPTaVYSp16aHAt6Gp/m9Xp9pFyJ+NQuR087n11mOEccvisO9Cqn34s8p7drHvAv3ytgM5xRLc5kpqsd30pOeqjsN8fL0t0WiYhGrTR/e9PTJvWyNispV57sZ6m09Tty79cjkC2ZwwukXNnrlqPc0ErPRL6MoockebgAAEABJREFUiZSzsaeaflrav4r4w3Fx3hZHz+3bt2fPnr1lyxaEO1YxvzM3N1cisY7tuZAVQOTECiInVqjVaqlUiqwAkjqxgsiJFUROrCBlJ1aQ1IkVRE6sIHJiBZETK4icWEHkxArSUMEK/Pe+RiR1YgYxI2AFSZ1YQeTECiInVhA5sYJUhbCCpE6scHd35xc3xR6rkDMpKYlfvB97rCMLkkggv0VWAJETK6xCToZhNBoNsgJI6sQKIidWEDmxgsiJFUROrCByYoVVDC4hqRMriJxYQeTECiInVhA5sYLYbLHCelInzqt+tW7dOiEhgeJX4aPyntTJyenUqVMIU3Bud/bu3VsqlWo396Bp/hMcg4ODEb7gLGfPnj39/PwMXVxdXcER4QvOckINCMTT70kPlC9fvl69eghfMDfydenSxd/fnz+2tbXt0aMHwhr8bbZ9+vThE2hAQECTJk0Q1ojcUHl4M41jmcJ8Kaqorc6LWLyY4jfE0i0nXNm3SWiV63Fxz9o07vrgRia/CjH3xkrDhcVm6F70asgsy9k50l6BYi44LVpDZeOsh+nJLCNBGmOL+3O6daCLlvNVUCO/sXatbxYZ3UHpLXG+4/rVhcRDSxBNIZ9KNu2+8EFiII6cP46PciotbdbTU6GQIby4dyXp4pGUSrXtmnX3RIIjgpyrx0VVDrWv/XEZhC/bF0W5eEg//dYfCYvQVaFDPz+VyWm8tQRaf+79/JEIW8QILWf8I6WLJ/5z85xcFVCO/r0vAQmL0DVbqPjI7axiMhcYMbLThS7IhJZTncNxKqvoq8pVscL34lhFB5lYCL/VLJHTXGg3ahfc5ia0nBR2GyQXBsdyrODbmgstJ6cDWQMUoknqxAdOu7O5wJDUaUaEf2+FlpPmLetWAMVQNIMERmg5WUia1pE6WQ2nsYZ2J4WsI3XCH/ZVIRr6AwXPgkQB8iD8GyrQZc9prCKzhRoCI3gtgViFzAXHVxSERYShXwKUndNnjD90eD8qPjp3bfks7qlJl2hTpuCVBBHk5JDZ39m7d2+h4iM+Pu7lyxRkKmIUKcLLSWmH2JlCcnLSrNmTevRq16lLi9lzv3v8OJZ3X7R4VveebfVrJ27dtr5124Zx8c+aNg+Fz4WLZrbv2ATcp04bO2PmhB/XLAP3v/4+CS57f90xdtyQ9h2adO3WCryePnui/65Hj2KGjfgSQvbu03H1jz+oVKprEZd79m4PXuCyctX3yCQEV1RoOUFMk7IgjUYzYtTgiOtXRgyf+PPaHS7Orl9/058XYPDgYWq1etPmn+D4xYvELVvXffP1KM8yXkcOnQWXMaO/+23/aTiQSqUPo6Pgb/bMJcHVa0ZGRixfsbBatRozZiwaP256Skry7DmT+e+CVDhk6IDqQSGLF63q3r3fiZNHli1fUDMkdO7speC7dcv+r78a8e53TtEUjX1ViNN/vBvw60OKgd+3Vs06cPpV+PCz5/7cs2fbt0PHOtg7DB0yZs7c79q27bx27YoqlYPate38ZgxgIo6Pf7Z65WYbGxs4dXBwXL9up4+PH79gca5aPXHyiNS0VCdHp917tsltbAaEhTMMA18nk8n+S6atNSNwuI9GMJXImxGQvHgtkU6bkBq1r9+4yp82bdLy6LGDEycNf/Hi+cb1ewqLxN+vLK8l0o35ePbsyf9WLr5952ZmZibv+DIlGeR8+PB+hQqVIQDv+Emr9vCH3hdImTT+Nlvd1Lx3D5+RkQ45KhRmho7Ozi764949Bwwd9gVo7O7uUVgkMoOlps+e/XPylFG9ew0YPGhYuXIVLl/5B8pR3iszM8Mw5v8Ih0SwZopgszWpR8XNzV2hUMyela8OwhjYttdvWN2wQZML/5w5dfoYJNa3Rvj7oV+rVw8Z+MU3/Cm8LnovOzv7zKxMVFxwItRtBc9sOWRS2VmuXMXs7OxSpcp4e+VNE4D2n7NTXhr6/eCvDx7e37p5/85dm6GCExpaDwrUoiNMS0stU/r1+PS/dXVdnkqVqv72+x79PgAnTv5x+PD++fOWo/eD0lk0hUX4mq1p/WO1a33wwQf1Fy2amZAQn5r6ct/+XeFf9T1y5AB4JSY+hyLwq8HD7ezsevf6XGGjWLlyCbjL5XIPj1KXL1+ANsabSyKUL1fx0iuvXbu38o7xCXHw2bZNJ2iZLPl+DuTAf5859dPa5W7uHlCU+voFgO/p08eiox+gd0drsxU6fTLTpk1DAnLpaLKTu8y/msO7X9K8WaucnBzIVJevWAS1mIYNm4b1Hwzu300Z5eTk/O3QMUhXwSlbtvzKVUugEC1Txksmkx8+cuDEicMdO372zz9nMzIz2rTuyMdWtWrws6ePN25aAxH6+QYM+3bc5cvnt+/YBHXdkJDa0IDZu3c7NEzPnf+rcaMW4eHD5TK5o4NjQkLc3l+3g9j16zd6x9u+8VeKi4esQk17JCBCz1FZPeaBX2W7jz7FfFIDsHX2A/+qdq3DBH1SwatCptoRSiyi9NML3lAxsSpUctGOokG4W4WsZ3CJVkwad6uQ9Qz94lgrGI1gckuFYAoimBEo4UcTiwFlDaPgWasZNc1Zwyh4XdFpHZktJUKLTPjMlrKWKQ3wpNi3O62nGkQzFP5lJ0LWYRPSjUawgmHT1jSDTHjESJ0caXeaC6HllMhpJLOK1ElLOFoidG4rtJxSKaXMsAozAhhMnNyEXhBL6LqXV3mbpGf4bxKfGJeVq0YftvFAwiK0nK36eIKt5OSuJwhrjm985ltRhMVAxVkAde13D2W2XO2W7n4VnBBeXD6ecPdiep2WrrVbuCLBEW154q0LotMStcPE33X7Ia5YW6yFx0YV3b1exIW6rj9GiirWtm/WTZzRMyJvi5OaqFK9sU4omAH5tgylO8xzpPKWoM6738JXjdb75MVDUY9iYtavXz9t2jT9BRSLuFfljOG36K7S/Ssket7X6PfCi+nhq0CiIvKkBicPIQqYhKScDNUzdy/cVrZ+E6uYfa1Wq/mR0NhjLXJKpfiviYysRE79PAXsIakTK0jqxAoiJ1YQObGCyIkVRE6sIHJiBZETK0i7EytI6sQKkjqxAv+9rxFJnZhB5MQKIidWEDmxgsiJFUROrCByYgWREyvc3bWL4iIrwCrkTExM1G/ogDfWkQVJJLnC7xIvBkROrCByYgWREyuInFhB5MQKIidWEDmxgsiJFVYxuIRhGM27LsFQsrEKOUlmixVETqwgcmIFkRMriJxYYT1yUhiv/dy5c+ecnBwQMisrS6VS8aLKZLIzZ84gTMG5odK4ceP4+Pjk5GSlUsmyLCgKrc+KFSsifMFZzr59+/r5+Rm62NnZffrppwhfcJbTzc2tdevWhi6+vr5t2rRB+IK5VahPnz7+/v78sVwu79q1K8IazOWE3LVLly5gs4VjLy8vqBwhrMHfZtuzZ0/IY6FaC6Um9tufvaWhcnz7s+jIbHUO934dEvqFhgv97sJ+3/dbDbqIFan5tY3f/ZJCozLuYfxJjYUt6qn1YfI/I00hWoLsHJiO35Zxcipq/HdRcp7cGX/3SkbZIIeKte1pibTQrzS46byneuWiPzT6HTRLsUa2E9YFz/9DGMZgqEvBmDl+7WgjC0Jr9+KBRzWSGVG639cwDu1K03wEnBExXv8z+qX5bqmAnByV9wMVuD0uL4IC3/L6VIPSkrNvX057HpMzaE5ZmYJBhVConDsWx6amqHuOKY8IlsTW2VEdBpXxKm9v1Nd42fk0JiMpjmhpifhUURzcEF+Yr3E5Lx5OUTgWmqIJItK4i7cqG4GFy6ivcTmV6RqJlGz8ZqFAzSj2drZRL+M9KqocxLFETgsFWhkcZ1w4q+ggsx6InFhB5Cx50BRHs8abl0TOkgfLgfnFeM2GyIkVxuXE3lSNK8bbnWT3eIum8LRGMtsSCAuCGk9uxlMnTVMUIunTUqF0fTDGMJ46WZbjECk+Sx4ks8UKImfJg9L2gZtSdlK0EFltpy4tNm1eCwd79m5v3vIDo2GW/jBvwBefoRLC1GljR43+qogAT548ato89NLlC+g/oB08QZlSdnKsoBWhqlWC+vYZiP4D02eMr1PnwzatOyJRadSouVqtQuJhEZltlSpB8If+A3fv3gI5kdg0b9YKiUrxyDl02BcKG8WC+Sv0LhMmDU9NfblyxYbo6AcHftt99dql+PhnAf6Bbdp06tih4LQCyGxXrlpy4thFOM7Kypo9d/K1a5fKli3fsX2+kIVFBdkXfC5cNHPV6u9/238ajo/88duB3/ZER0dBJM2afty1S8+32rlyc3PX/bzywj9nnj+PDwoK6dzxs3r1GvJeUCgMCAuHx9m4aY1CoagT+uGQb0a7ubk/fBj1xZc95s5eumjJLGdnl7VrfoHMNiMjffGiVXBVWnrajz/+cOjwficn59Dadb8cOLR06TL6r1u8ZPbvB3+FSBp91OzboWORSVCFFoXFM862aeOWV65ezMzM5E+VSuXlyxdaNPsEjv+3cvGlS+eHfTtu3txlIMAPy+Zf+OdsEVEtWjwTCphFC1fNnL4oOuYB/L56r8KiOnJI+zlm9He8lsdPHJm/YHrFCpW3bTkw8Itvdu/ZtmLlYvQ2li1fACE7d+q+betvjRs1nzp97J9/neC9pFLpjh2baJre9+uJjev3RN6M2LDxR94dPjdtWdv9s76jRk42jA1ejvETvn2RlLhk8eqhQ8Y8T0wYP/Fb/aTE9RtWBwfXAq/PuvX5dd/Ok6eOIpPgEKJM6VGhaNOsCI0bt1j+v0V/nzn5Sav2cHrm7GmWZZs0aQnH3303Nysr07OMFxzXDAk9cuTAxUvn6tVtYDSeFy8ST50+Nm7s1Kq6vHfwoG/Pnf9L7/uOUR06tC84uObwYePh2MXFdUD/8AWLZvTp9TkcF3b/OTk5fxz9vVfPsA7ttbMeoAy+efP6ps0/ga58AG9v3z69P9ce2TtA6rx37zZ6ZdmuE1qv26e9C0QIb+Ht2zc3rt/t5xeAtHNj/Hfu2pKcnMT7ws23bNGaP9j76/bIyGuQhSBT4DhTqkKmWuAh0wipUfvvM6d4Oc+ePV271geurm78N+/du/2fi2cfP47lA3t6ehcWT1zcU/j09w/Uu1SqVPX+/Tvo1UO8NSp4jW7+e71f3y/1LjVr1gHHG5HX9Nq8CcijUqlAJ70LPM7hIwdS01KdHJ3gtGLFKnovBwfHzMwM/WnFClXejPDBg/u2tra8lrowlSdPnIV0NVv4rB4Uog/p5OgMLxMqJgqxCmk4jjNNUkiLK/63CLJZhmHOX/ibLw/gdxw/cRhU9r4cOCQkJNTB3gFK2SIiSU17CZ+2Clu9CxTJebf0blGBKmq1GkpB+DN0T0lJLuJ7ocBDuhpAAfeU5CReziKKXplc/qYj6C2X2xR2CWO2hemLLV6QE4ofyBtlMpk2p22szWnv3b9z586/ixauhMTKB4MfzsO9VGGRwKsKn8qc1yUIxPgAABAASURBVAt9Q+7KH7xjVDY2NpAsPm7ZtlH+tOjl6YMKx83dAz5HjZwEmaqhe6lSZdB7YWtrl52dBb8DlLio2KEQjczcfQ1vMfzQFy+ey8lRNqjfGH5TcITaIHzqf/SYmIfwVzagXGGRlNGVi1BuVdJlbpDOLl/5ByqNJkVVrlzF9Ix0KJb4U4gE8vBSpUqjwvHx9pPrEpn+KkjNkEHxT/EeVK5UFTKqu/duV6lcDU4fPYpZsnTO0G/GyI0lZZPRGoVM7VEx3SwEFaIbN65eufIPXwkCoDkhkUh27NwMtXZ4pOUrFkLFIT4hrrAYPDxKBQXV2LBhNZSOUKLMmj1Jfx9FRAW/EVwIdelrEZeh9vjlF0Og8IYWAiSOyMiIGTMnjBwdXtg4Yx6QLaz/YKj7QHgICXXa0WO/BoMUel9CQ+tBQl+zZhnUJ8AGBFElPk/w9y+LionCSkLjcmp7VEw3C0EGm/A8PleTC6mTd4GW1qSJs27djuzYqdnEySOg2dChw6dQ5es/oNAZ7RPGzwCTwqDw3m3bN4JKB1Qy+VspOqrevT6H9uh3U0ZlK7OrVw9Zs3rrjRvXOndtCapAMTZr5pK3Jose3fuNGT1l2/YN7Ts2gSYQZM6jRk1G7wu8eYsWrGQ5dsrUMWPHDbFRKObO+UGAvVyMTznaODOGY6muw/0RwfLYOC3q4/6eFUPs3vQiPSolEFMHl0CPCn6DEdp3aFKY17hx0xo2aIJKChyUkaZYhbQ9KtjJuW3bb4V56Vu3JQXWpIYKzSBOgA5PYQHLA8KdwqxC2gRKKHEU0e4kQ79KHoWP5CPjMi0V6O8qzHJIGiolD44ttCQkcmKFFbU7rQErandaAySzxQrjVSSpjKYlJHlaKNpB7RrjlaHC5ITsltgRLBQoB129CjHnGXUtW8NOmUZSpyVy42yyRIrcPI0bmY3LGdrMXSpFx7bEIoKFcetccjljPZ08RS2Auva7B3Jb1OnrQof2EIQk5k7qmd2JdVq7hjYrdMDwW5Yn3jjzYWYqCx0smty3rahL6do2hS8enBfA0IWhOE2eE1VkQ5e3H3Mc92bX7ZvRGrhzxpcSNvZdlK6KwbLvGph3hEt4C43xYG8+sm7ul+G3GA1TwIWRaIfKwncEBtl+0t8LFc7bt8VRZauu/pWqyig61OtliE2x3FMmWSveO/L09Iw7d27XqVPnTa98kRu7HX61YvT2L3knOH7Ebr7f/J2icCyFQj7yeGswnHc50nPz5s2FCxdu3LgR4Y5VmBFyc3MFGEVnCVjFQ2o0GiInPqjVaiInPpDMFiuInFhB5MQKKDv5ie/YQ1InVhA5sYLIiRVETqwgVSGsIKkTK4icWIH/ZsmIyIkZpOzEClJ2YgWREyuInFhB5MQKUhXCCpI6scLBwUGhKGErB70fViFnampqMa7obMlYRxYkkeiX1ccbIidWEDmxwirkZBhGo9EgK4CkTqwgcmIFkRMriJxYQeTECqsYjUDkxAqS2WIFkRMriJxYQeTECiInVliPnDgvE9WtWzd+d/Xs7GzovnZzc4OHVSqVx48fR5iCc0MlODg4ISHh+fPn6enpKpUqLi4uPj7excUF4QvOcvbu3dvXN9/mx9BT1qlTJ4QvOMsZGBjYoEEDw+2afHx8OnfujPAFc6tQnz59/P3zNpUFXdu1a/fe21mXCDCX09PTs2nTpvyxt7c33kkTWYPNtkePHlCCQtJs0aKFs7MzwhoLaqg8jc4+vSMhK12Tk8VR2i0K81bxLbDUNO/+yhEOON1TaE8ZBvFDggwXbNat7gxXcwxD6UMaBqBfLTKtj9NwsWLD2zD8qSQSxMiQu7esU7gvshgsRc7719KOb3vuXErm6iOjEWO4qnTees8chai8Fbu1bgY/uMFqza+P9ZIXQlFrPPNfZrB8Nse7Gi52TTGUMl0d/ygrV4UGz7WU5fItQs5Tu+Nvnc/oN6U8KoH8cyQu6mpW+HyLUNQiys7bFzI6D/FGJZO6n3i6lJZtmhODLADx5Ty84ZncBjm4luApQfU7l05PsgibsPhypr5QyxUle/Kls6sMqm5PozKR2Ijfo6JWIrW6xO93ptFwnEb8tEE2fCw2LGHrdyJn8aDdwNYC9LQAOamiG4glA+3uVRaQPi1ATg6HjZk5ba1S/McQX06dPa/EJ8+CG4uJhPhycjqDKiIUB5ZQduKQOrWQqhDSVSIQXeLl1GYvFvBSWkBmq22Al3gzglZJllSFcIHjSGarg5FSrCW0wP8blInb7JoJ8eXUqDlNLhY1WwvobLQMq5BF2Dv/MxZQAbCAN4rjh4oUA7/u2zl3/lQkDpwltLawqgrdvXsLiQZFrEJaGCltag3/0aOY9RtWR1y/AuakatWCe3zWr3r1kOEjB12/fhV8jx49+OPqLRUrVIZgS3+Yd+/+bYaRBAQEhvUfXDMkFAJMnTaWYZjSpT2379g0fdqCRh81S05OWrlqyc1/ryuVyjp1PuzXZ6Cvr79p92QZJYb4ma1GzWpyTSh2VCoVKAd6zJ+3fPHCVRJGMmnyCJBh6ZI1VaoEffxx21MnLoOWKSnJQ4YOKFWqzJoft/1v+XoXZ9eZsyZmZWVBDFKp9GF0FPzNnrkkuHpNjUYzYtRgeDlGDJ/489odEPLrb/o/ffYEmQZnCTVbCyg7Tewge/LkEUjVtUtP0KxcuQpTp8ybPn3hm9M3d+3eKpPLR4+a7OXp7ePjN2b0lOzsrP0HdiGdVTE+/tn0qQvq12/k7OwSGRkB6XjihJl1P6jv6ur2VfhwRyfnPXu2IVMhqVOLiR1k3t6+oMG8BdO2bP355s3rNE1DFmpvb18gGCS+ChUq69cMt7Oz8/Xxv3fvNn/q71fWxsaGP468GQHptVbNOvwpiB1So/b1G1eRKVhIJ0LJqwrJ5fIfvv/p4KF9u/dsW/fzSi8vn7B+g1q2bFMgWHLSCxDe0MVGocjKzuKPIeHq3TMy0tVqddPmoYaB4Y1BpkDMCHnQEorWmJZP+fkFQJY4ICz86tWLh48cmDNvin9AIOS9hmFs7eyUOUpDl+ysLB9vvzdjc3NzVygUs2d9b+jI0AwyCe3IefGzOvHvgM3lWFOqto8fx4KEcAC5JRR+06bOhxxVn4vqqVSx6u3bNyHZ8adp6Wmxj6LLljUyVr1cuYrZ2dlQaYJMm/+DSm/58pWQSWinXIhvRxBfTlN7O9PSUhcsnLFq9dInTx+DtFu3rYd6UFC1GkhXrIKEV69dgrpS+/ZdMzMzFi+ZnZAQHxPzcO68KTZymzatjUy9rl3rgw8+qL9o0UwImZr6ct/+XeFf9T2ie2NKHOLLaepYBGhojhwx8fiJw337de4X1jUy8tqSxauhWQle7dt2gXdjzNhvHjy87+PtC5Xe6OioHr3aQcMGfH9YuhYqREbjnDt7aePGLWbMmtCpS4u9v25v0aJ1ly49kClYiNFZ/ClHm2fFqtVst5FlUUlm47SoTuHePpVEnppB+juLDUsw2lpAzZaGPxwGl5CBmXlwGHRfIzJsWgfLgpwlfqyQFguwDFlAQ4WhMMhsLQSLGMnHWsAYuP8IZxHjMknNtpggkxryoBhEsSSzLR4sYakLylJsKv8BqMyRzFaLdhR8yS87tfM7SWZLKF6InFhhEcOmabrkr/QI3deM+FuEiv87yu0QLS3xZSctQVKZiQMYzHEbSGy8AhXZaSV7G4W7V5KgWlvGT/yFy8SXs2GHUpDfXj35ApVYIv9K8ywvRxaApSyAunJ0VPna9h+2KYNKGtsXRHmWVbQbaBFLRFqKnBqN5ucpMZpcJFNQahX3ZuUIzPRg2mVoWsO+7n6hqLz71zXhXz/La/fXwz4osPOz+R9WH0znlc9RvxgxTVF5V+UfeimRUVwuq8xiXUpLe44xdQaEubCsbXEuH3/x5J4yM01NGY6L1E1s1rbTWcRIKMPJoK+XgqZ1RtNXQsMpb5qgdCsN5+bmpqenu7q65CkEVkV+MQNdnChvwem8+dP84tP69+Z1mFdX8UhllJ0jHdrKtZS3Ba31ifMuR3r+/fff+fPnb9q0CeGOVZgRIHXqZzfgDZETK4icWEHkxAoiJ1YQObHCKh5SrVZLpSV794B3hKROrCByYgWREyuInFhBqkJYQVInVhA5sQL/zZKRNWW2ViEnyWyxgsiJFUROrCByYgVpd2IFSZ1YQeTECoqiZDIZsgKspexkWSyWLnob1pEFSSRvLhaPJUROrCByYgWREyusQk6GYTQa8RcuEACSOrGCyIkVRE6sIHJihVWMRoDuFP3+OHhDarZYQTJbrCByYgWREyuInFhB5MQKzJeJ6tatGzRR0tPTVSqVs7Mz1G+VSuXJkycRpuCcOkeMGPHgwQP98n7Z2dnw6ePjg/AFZzNC3759XV1dDV0gK/r4448RvuAsZ61atUJDQw1LEy8vr86dOyN8wdzIFxYWVrp0af4YdG3YsKGnpyfCF8zlrFy5cp06dfgECkL26GHaFsglDvxN8P369fPz82NZtmbNmv7+lrKOsJmwrIbKxT8Sn9zPyXyp1rAoN38XCL/atKELpd2xg9Jo3nL/ECwzMwsaKg4ODmCLfzMePnLdOseU4XLURkMi7TrT2iWopTaUs7u0fIhdhRAnZDFYhJzHtz2Lvpmdo+SgTUExNMVQEoYueF+GC3zzULrVpt8+HJrSLTqtP6OMbC9FvfrP0MtoSLgRhmY5ls1lNfDW6YwTds50w05uFWqIr6vIch7eFBd9PRMxyM7FxrOKm0xe8qYeJD9NT4x+qc7Klcqo9l+V8QqwQ+IhppxrJj5kNVypci6uvhaUX7030VeeZSbllA6QdRvmh0RCHDnvXk09tjnRydPWt3pphBd3/oplKO7LOeWQGIggZ2qSavOsR1Wa+DEWsMuTOXhw+Smn0gycWRYJjtBy3r6UdnLH82rNRXhUIYm5+kyZpgqfL3QaFbTdCY2/E9vw1xIIqOUlt5evmxKNhEVQOaHu4+hlj6yDsqGeKiX7x6Y4JCDCyXnw52csi/yCPJDVULaO1/2ITCQgwskZHZlVpqIbsiZs7GVSG8m2+bFIKASS88imZxSDXL0dkUUSEXl89Hd1MzJTUHHjWc09OUG4EdsCyRl7O8vewxZZH46uCkZCHd0SjwRBIDlV2cg/GDeLwTsit5M9upOFBEGIsUJn9ifS5vyemEc3jp5a+/jJLXs7lyqVGn7cdKCNjdZwunnHRGhY16rxyY69M3Jysvx9q7dtNcTfN4i/6vcjyy9fPySX2dYMblXK3YxmOWdvh/g7Au0dLETqjI/JZiTm+qIXSY9/3DBUrc4ZMmht/17z4xLur/r5K41G29NB05LYx5FXIg4PC98wZ8qfEqls+94Z/FXnLu45d3F3l7Zjhg1e7+bidezUOmQ2XL0doEqf/VKJzI8Qcmana8xnz7t6/YiEkYb1nF/aI6BMqcBuHSc9jbt78/afvC8kyu6dJ7u5ejO4vBz9AAAD50lEQVSMpFZwq8QXseAC7mfO7wyu1jw4qJmtrWOdWu3KB4YicwIdf9F3cZFTnUtJpOaSE3JaX5+qdnbO/Kmri6ebq090bAR/WsojQC7Pq4LZ2DjAZ1Z2Gtg1XyQ/Ll3qtXHKx6syMisU9TJZCGOqIONsofOYo5B5yFZmPH56C5oZho5p6Un8AUUZeV+VOZksq9HLDMhk5t3wGF4gmhNi2TEh5JRIUY7aXNMrHRzcyvqHtGo2yNDRzq6oDlQbuR1NM2r169wvR2XmmieHHNyEyAiFkFNhR2cnmGuKiFfpCleuHwoMqKkf7R7//KGHW1E1VYqiXJw9Yx5FNm6Q53L77llkTiBl+lUQYpSCEK9M6QC5RmOurKZR/Z7QUXPg8PcqlfJ5Yuzvf6xYvKJXXEJU0VfVCGoReesUGIPg+OTfm2Kf3ERmI+VZGqKQg6sQ42aEkLNhRw/WbPO3oGo6esg2mVSxdHX/Bcs+exhztVunSW+t2rRoPKBu7Y77Di2GQheSZofWw5GuhENmIPlpho2duaoOBRCo+3rV2Af2bgpfqzQM3TweXS7ItvUAL2R+BDLy+VdVZLzIRtZHdoYKsUgYLZFgEwLbhHmtHB31Mj7NuYzxTpXniTHL1nxRyNUUQsazEMgw23/yLSo+Js9ubtQdGjaQjYEt4k2v6lWadO/yHSqE2GtxzmWEm3Up3FihAz8+ffogp0pT49MKwCyXmvbcqFdmVpqdrfGXQCaztX9lQCgWklOeFealUufIpHJk5B4UYCs2folSde+vp0O+L4+EQtChX6vGRjmWtvOuWgpZB7dORfsE2nYIFyinRQKPFeo7wfflU0EHW4hI9NU4mYwWUksksJz2LvL67V1vnRB6fJvwPL6RoExVDpwViIRFhGHTz58ody5+EvQxtsMzYyPictJVg+YIrSUSa1LDtT+Tz+1PdvF28KrqjvDi3plHUA8eNNdqJjXwZGcoN816CjdQpoqbUykcBt/GXIvLSFS6ekp7jRVtUrDIEwIP/PT08d1shqHtPRQ+1UpkjTctKSv+TpIqMxf64lr286go6ixPi5iue3Dt0ycPlOocjmago5umJDRN0bTkHe2chkaG18ccC32dXL5g8KSUPsCbB/zZG1N0Od1sX4P44VzDajQqVpPLsmrtfG25LfXBJy41PnJFYmNBk+nBHnbp2MuE2JysjNycTM5wlyn9j0y9EkV/1zSj7X4y9M1zfzVZm5eLevVu5AtJ6Xy515JKJFRuLqcPoJ1jj3Td7wa3Ac0PiuEkUsrBVRIYbBvcQHwV9WC+iJu1YRVLLFoPRE6sIHJiBZETK4icWEHkxIr/AwAA//8xj7v9AAAABklEQVQDANmoVLu4MWPxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def should_continue(state: ResumeState) -> str:\n",
    "    \"\"\"\n",
    "    Conditional edge to determine if workflow should continue or end\n",
    "    \"\"\"\n",
    "    # Check if any previous step failed\n",
    "    if state.get('parse_error'):\n",
    "        return END\n",
    "    if state.get('extract_error'):\n",
    "        return END\n",
    "    if state.get('validation_error'):\n",
    "        return END\n",
    "    if state.get('store_error'):\n",
    "        return END\n",
    "    \n",
    "    return \"continue\"\n",
    "\n",
    "def create_resume_workflow():\n",
    "    \"\"\"\n",
    "    Create the LangGraph workflow for resume transformation\n",
    "    \"\"\"\n",
    "    # Initialize the state graph\n",
    "    workflow = StateGraph(ResumeState)\n",
    "    \n",
    "    # Add nodes (agents) to the graph\n",
    "    workflow.add_node(\"parse\", parse_agent)\n",
    "    workflow.add_node(\"extract\", extract_agent)\n",
    "    workflow.add_node(\"validate_enrich\", validate_and_enrich_agent)\n",
    "    workflow.add_node(\"store\", store_agent)\n",
    "    \n",
    "    # Define the flow: parse -> extract -> validate_enrich -> store\n",
    "    workflow.set_entry_point(\"parse\")\n",
    "    workflow.add_edge(\"parse\", \"extract\")\n",
    "    workflow.add_edge(\"extract\", \"validate_enrich\")\n",
    "    workflow.add_edge(\"validate_enrich\", \"store\")\n",
    "    workflow.add_edge(\"store\", END)\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Create the workflow\n",
    "resume_workflow = create_resume_workflow()\n",
    "\n",
    "print(\"âœ“ LangGraph workflow created successfully!\")\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "display(Image(resume_workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c053da",
   "metadata": {},
   "source": [
    "## Main Execution Function\n",
    "\n",
    "Function to process a resume through the complete workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11927963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Main execution function defined!\n"
     ]
    }
   ],
   "source": [
    "def process_resume(file_path: str, db_path: str = \"resume_ats.db\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a resume through the complete workflow\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the resume file (PDF, DOCX, or TXT)\n",
    "        db_path: Path to the SQLite database\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with processing results and status\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸš€ RESUME TRANSFORMER WORKFLOW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Determine file type\n",
    "    file_extension = Path(file_path).suffix.lower().replace('.', '')\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state: ResumeState = {\n",
    "        'file_path': file_path,\n",
    "        'file_type': file_extension,\n",
    "        'raw_text': None,\n",
    "        'parse_error': None,\n",
    "        'structured_data': None,\n",
    "        'extract_error': None,\n",
    "        'validated_data': None,\n",
    "        'validation_error': None,\n",
    "        'database_id': None,\n",
    "        'store_error': None,\n",
    "        'status': 'processing',\n",
    "        'messages': []\n",
    "    }\n",
    "    \n",
    "    # Run the workflow\n",
    "    try:\n",
    "        # Execute the workflow with the store agent receiving db_path\n",
    "        final_state = resume_workflow.invoke(initial_state)\n",
    "        \n",
    "        # Need to manually call store_agent with db_path since LangGraph doesn't support extra params\n",
    "        if not final_state.get('validation_error'):\n",
    "            final_state = store_agent(final_state, db_path)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ðŸ“Š WORKFLOW SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for message in final_state['messages']:\n",
    "            print(f\"  {message}\")\n",
    "        \n",
    "        print(f\"\\n  Final Status: {final_state['status'].upper()}\")\n",
    "        \n",
    "        if final_state.get('database_id'):\n",
    "            print(f\"  Database ID: {final_state['database_id']}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return {\n",
    "            'success': final_state['status'] == 'completed',\n",
    "            'database_id': final_state.get('database_id'),\n",
    "            'candidate_name': final_state.get('validated_data', {}).get('contact', {}).get('name'),\n",
    "            'messages': final_state['messages'],\n",
    "            'state': final_state\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Workflow failed: {str(e)}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'messages': [f\"Fatal error: {str(e)}\"]\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Main execution function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efbe5f",
   "metadata": {},
   "source": [
    "## Test the Workflow\n",
    "\n",
    "Let's test the workflow with a sample resume from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dd8022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ RESUME TRANSFORMER WORKFLOW\n",
      "================================================================================\n",
      "ðŸ” PARSE AGENT: Processing data/candidate_042.pdf\n",
      "   âœ“ Extracted 714 characters\n",
      "ðŸ¤– EXTRACT AGENT: Analyzing resume with GPT-4o-mini\n",
      "   âœ“ Extracted data for: Ryan Nelson\n",
      "âœ¨ VALIDATE & ENRICH AGENT: Processing data\n",
      "   âœ“ Calculated 1.7 years of experience\n",
      "   âœ“ Standardized 6 skills to 6 unique skills\n",
      "ðŸ’¾ STORE AGENT: Saving to database\n",
      "   âœ“ Stored candidate with ID: 3\n",
      "ðŸ’¾ STORE AGENT: Saving to database\n",
      "   âœ“ Stored candidate with ID: 4\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š WORKFLOW SUMMARY\n",
      "================================================================================\n",
      "  âœ“ Successfully parsed 714 characters from PDF\n",
      "  âœ“ Successfully extracted structured data using GPT-4o-mini\n",
      "  âœ“ Successfully validated and enriched data\n",
      "  âœ“ Successfully stored candidate with ID 3\n",
      "  âœ“ Successfully stored candidate with ID 4\n",
      "\n",
      "  Final Status: COMPLETED\n",
      "  Database ID: 4\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set your OpenAI API key\n",
    "# Uncomment and set your API key here:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"\n",
    "\n",
    "# Test with a sample resume - trying Image_10.pdf with OCR support\n",
    "sample_resume = \"data/Image_10.pdf\"\n",
    "sample_resume = \"data/candidate_042.pdf\"\n",
    "\n",
    "# Process the resume with OCR\n",
    "result = process_resume(sample_resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23e702",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "Check the processed data and query the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "264e77a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Resume processed successfully!\n",
      "\n",
      "ðŸ“‹ CANDIDATE PROFILE\n",
      "================================================================================\n",
      "\n",
      "ðŸ‘¤ Contact Information:\n",
      "   Name: Ryan Nelson\n",
      "   Email: N/A\n",
      "   Phone: N/A\n",
      "   Location: N/A\n",
      "\n",
      "ðŸ’¼ Experience: 1.7 years\n",
      "\n",
      "ðŸ› ï¸ Skills (6):\n",
      "   1. C Programming\n",
      "   2. Java\n",
      "   3. Machine Learning\n",
      "   4. Python\n",
      "   5. SAP\n",
      "   6. Software Engineering\n",
      "\n",
      "ðŸŽ“ Education:\n",
      "   â€¢ B.Tech in Computer Science\n",
      "     JNTU (2019)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# View the extracted and validated data\n",
    "if result['success']:\n",
    "    print(\"âœ… Resume processed successfully!\\n\")\n",
    "    \n",
    "    validated_data = result['state']['validated_data']\n",
    "    \n",
    "    print(\"ðŸ“‹ CANDIDATE PROFILE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Contact Information\n",
    "    contact = validated_data.get('contact', {})\n",
    "    print(f\"\\nðŸ‘¤ Contact Information:\")\n",
    "    print(f\"   Name: {contact.get('name', 'N/A')}\")\n",
    "    print(f\"   Email: {contact.get('email', 'N/A')}\")\n",
    "    print(f\"   Phone: {contact.get('phone', 'N/A')}\")\n",
    "    print(f\"   Location: {contact.get('location', 'N/A')}\")\n",
    "    \n",
    "    # Experience Summary\n",
    "    total_exp = validated_data.get('total_years_experience', 0)\n",
    "    print(f\"\\nðŸ’¼ Experience: {total_exp} years\")\n",
    "    \n",
    "    # Skills\n",
    "    skills = validated_data.get('skills', [])\n",
    "    print(f\"\\nðŸ› ï¸ Skills ({len(skills)}):\")\n",
    "    for i, skill in enumerate(skills[:10], 1):  # Show first 10\n",
    "        print(f\"   {i}. {skill}\")\n",
    "    if len(skills) > 10:\n",
    "        print(f\"   ... and {len(skills) - 10} more\")\n",
    "    \n",
    "    # Education\n",
    "    education = validated_data.get('education', [])\n",
    "    print(f\"\\nðŸŽ“ Education:\")\n",
    "    for edu in education:\n",
    "        print(f\"   â€¢ {edu.get('degree', 'N/A')} in {edu.get('field', 'N/A')}\")\n",
    "        print(f\"     {edu.get('institution', 'N/A')} ({edu.get('graduation_year', 'N/A')})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"âŒ Resume processing failed!\")\n",
    "    print(f\"Error: {result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1967d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ RESUME TRANSFORMER WORKFLOW\n",
      "================================================================================\n",
      "ðŸ” PARSE AGENT: Processing data/1901841_RESUME.pdf\n",
      "   âœ“ Extracted 4568 characters\n",
      "ðŸ¤– EXTRACT AGENT: Analyzing resume with GPT-4o-mini\n",
      "   âœ“ Extracted data for: Anuva Goyal\n",
      "âœ¨ VALIDATE & ENRICH AGENT: Processing data\n",
      "   âœ“ Calculated 0.2 years of experience\n",
      "   âœ“ Standardized 22 skills to 22 unique skills\n",
      "ðŸ’¾ STORE AGENT: Saving to database\n",
      "   âœ“ Stored candidate with ID: 5\n",
      "ðŸ’¾ STORE AGENT: Saving to database\n",
      "   âœ“ Stored candidate with ID: 6\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š WORKFLOW SUMMARY\n",
      "================================================================================\n",
      "  âœ“ Successfully parsed 4568 characters from PDF\n",
      "  âœ“ Successfully extracted structured data using GPT-4o-mini\n",
      "  âœ“ Successfully validated and enriched data\n",
      "  âœ“ Successfully stored candidate with ID 5\n",
      "  âœ“ Successfully stored candidate with ID 6\n",
      "\n",
      "  Final Status: COMPLETED\n",
      "  Database ID: 6\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set your OpenAI API key\n",
    "# Uncomment and set your API key here:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-api-key-here\"\n",
    "\n",
    "# Test with a sample resume - trying Image_10.pdf with OCR support\n",
    "sample_resume = \"data/Image_10.pdf\"\n",
    "sample_resume = \"data/1901841_RESUME.pdf\"\n",
    "\n",
    "# Process the resume with OCR\n",
    "result = process_resume(sample_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6869b2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Resume processed successfully!\n",
      "\n",
      "ðŸ“‹ CANDIDATE PROFILE\n",
      "================================================================================\n",
      "\n",
      "ðŸ‘¤ Contact Information:\n",
      "   Name: Anuva Goyal\n",
      "   Email: anuvagoyal111@gmail.com\n",
      "   Phone: +91 9520349542\n",
      "   Location: N/A\n",
      "\n",
      "ðŸ’¼ Experience: 0.2 years\n",
      "\n",
      "ðŸ› ï¸ Skills (22):\n",
      "   1. C\n",
      "   2. C++\n",
      "   3. CSS\n",
      "   4. Code Blocks\n",
      "   5. Data Structures\n",
      "   6. Google Colab\n",
      "   7. HTML\n",
      "   8. Java\n",
      "   9. JavaScript\n",
      "   10. Jupyter Notebook\n",
      "   ... and 12 more\n",
      "\n",
      "ðŸŽ“ Education:\n",
      "   â€¢ B.Tech. in Electrical Engineering\n",
      "     Dayalbagh Educational Institute (2023)\n",
      "   â€¢ None in None\n",
      "     St. Clareâ€™s Senior Secondary School (2019)\n",
      "   â€¢ None in None\n",
      "     St. Clareâ€™s Senior Secondary School (2017)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# View the extracted and validated data\n",
    "if result['success']:\n",
    "    print(\"âœ… Resume processed successfully!\\n\")\n",
    "    \n",
    "    validated_data = result['state']['validated_data']\n",
    "    \n",
    "    print(\"ðŸ“‹ CANDIDATE PROFILE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Contact Information\n",
    "    contact = validated_data.get('contact', {})\n",
    "    print(f\"\\nðŸ‘¤ Contact Information:\")\n",
    "    print(f\"   Name: {contact.get('name', 'N/A')}\")\n",
    "    print(f\"   Email: {contact.get('email', 'N/A')}\")\n",
    "    print(f\"   Phone: {contact.get('phone', 'N/A')}\")\n",
    "    print(f\"   Location: {contact.get('location', 'N/A')}\")\n",
    "    \n",
    "    # Experience Summary\n",
    "    total_exp = validated_data.get('total_years_experience', 0)\n",
    "    print(f\"\\nðŸ’¼ Experience: {total_exp} years\")\n",
    "    \n",
    "    # Skills\n",
    "    skills = validated_data.get('skills', [])\n",
    "    print(f\"\\nðŸ› ï¸ Skills ({len(skills)}):\")\n",
    "    for i, skill in enumerate(skills[:10], 1):  # Show first 10\n",
    "        print(f\"   {i}. {skill}\")\n",
    "    if len(skills) > 10:\n",
    "        print(f\"   ... and {len(skills) - 10} more\")\n",
    "    \n",
    "    # Education\n",
    "    education = validated_data.get('education', [])\n",
    "    print(f\"\\nðŸŽ“ Education:\")\n",
    "    for edu in education:\n",
    "        print(f\"   â€¢ {edu.get('degree', 'N/A')} in {edu.get('field', 'N/A')}\")\n",
    "        print(f\"     {edu.get('institution', 'N/A')} ({edu.get('graduation_year', 'N/A')})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"âŒ Resume processing failed!\")\n",
    "    print(f\"Error: {result.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f05ba1",
   "metadata": {},
   "source": [
    "## Query Database\n",
    "\n",
    "Let's query the database to see the stored records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a4b2706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š CANDIDATES IN DATABASE\n",
      "====================================================================================================\n",
      "ID    Name                      Email                          Experience   Created             \n",
      "----------------------------------------------------------------------------------------------------\n",
      "5     Anuva Goyal               anuvagoyal111@gmail.com        0.2          2025-12-25 10:51:09 \n",
      "6     Anuva Goyal               anuvagoyal111@gmail.com        0.2          2025-12-25 10:51:09 \n",
      "3     Ryan Nelson               N/A                            1.7          2025-12-25 10:48:43 \n",
      "4     Ryan Nelson               N/A                            1.7          2025-12-25 10:48:43 \n",
      "1     Elijah Harrison           example-email@example.com      21.2         2025-12-25 10:46:52 \n",
      "2     Elijah Harrison           example-email@example.com      21.2         2025-12-25 10:46:52 \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Total candidates: 6\n",
      "\n",
      "ðŸ› ï¸ Skills for candidate 5 (Anuva Goyal):\n",
      "C, C++, CSS, Code Blocks, Data Structures, Google Colab, HTML, Java, JavaScript, Jupyter Notebook, Keras, MATLAB, MS Office, NumPy, OpenCV\n",
      "... and 7 more\n"
     ]
    }
   ],
   "source": [
    "def query_database(db_path: str = \"resume_ats.db\"):\n",
    "    \"\"\"Query and display all candidates in the database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all candidates\n",
    "    cursor.execute('''\n",
    "        SELECT id, name, email, phone, location, total_years_experience, created_at\n",
    "        FROM candidates\n",
    "        ORDER BY created_at DESC\n",
    "    ''')\n",
    "    \n",
    "    candidates = cursor.fetchall()\n",
    "    \n",
    "    print(\"\\nðŸ“Š CANDIDATES IN DATABASE\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    if not candidates:\n",
    "        print(\"No candidates found in database.\")\n",
    "    else:\n",
    "        print(f\"{'ID':<5} {'Name':<25} {'Email':<30} {'Experience':<12} {'Created':<20}\")\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            cid, name, email, phone, location, exp, created = candidate\n",
    "            # Handle None values safely\n",
    "            name_str = (name[:24] if name else 'N/A')\n",
    "            email_str = (email[:29] if email else 'N/A')\n",
    "            exp_str = str(exp) if exp is not None else 'N/A'\n",
    "            created_str = (created[:19] if created else 'N/A')\n",
    "            print(f\"{cid:<5} {name_str:<25} {email_str:<30} {exp_str:<12} {created_str:<20}\")\n",
    "        \n",
    "        print(\"-\"*100)\n",
    "        print(f\"Total candidates: {len(candidates)}\")\n",
    "    \n",
    "    # Get skills for latest candidate\n",
    "    if candidates:\n",
    "        latest_id = candidates[0][0]\n",
    "        cursor.execute('SELECT skill FROM skills WHERE candidate_id = ?', (latest_id,))\n",
    "        skills = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        if skills:\n",
    "            print(f\"\\nðŸ› ï¸ Skills for candidate {latest_id} ({candidates[0][1]}):\")\n",
    "            print(\", \".join(skills[:15]))\n",
    "            if len(skills) > 15:\n",
    "                print(f\"... and {len(skills) - 15} more\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "# Query the database\n",
    "query_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268cbde8",
   "metadata": {},
   "source": [
    "## Batch Processing\n",
    "\n",
    "Process multiple resumes from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8842477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_resumes(folder_path: str, db_path: str = \"resume_ats.db\", limit: int = None):\n",
    "    \"\"\"\n",
    "    Process multiple resumes from a folder\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing resume files\n",
    "        db_path: Path to database\n",
    "        limit: Maximum number of resumes to process (None for all)\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "    resume_files = list(folder.glob(\"*.pdf\")) + list(folder.glob(\"*.docx\")) + list(folder.glob(\"*.txt\"))\n",
    "    \n",
    "    if limit:\n",
    "        resume_files = resume_files[:limit]\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ BATCH PROCESSING: {len(resume_files)} resumes\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'details': []\n",
    "    }\n",
    "    \n",
    "    for i, resume_file in enumerate(resume_files, 1):\n",
    "        print(f\"\\n[{i}/{len(resume_files)}] Processing: {resume_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            result = process_resume(str(resume_file), db_path)\n",
    "            \n",
    "            if result['success']:\n",
    "                results['successful'] += 1\n",
    "                results['details'].append({\n",
    "                    'file': resume_file.name,\n",
    "                    'status': 'success',\n",
    "                    'candidate': result.get('candidate_name'),\n",
    "                    'db_id': result.get('database_id')\n",
    "                })\n",
    "            else:\n",
    "                results['failed'] += 1\n",
    "                results['details'].append({\n",
    "                    'file': resume_file.name,\n",
    "                    'status': 'failed',\n",
    "                    'error': result.get('error')\n",
    "                })\n",
    "        except Exception as e:\n",
    "            results['failed'] += 1\n",
    "            results['details'].append({\n",
    "                'file': resume_file.name,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ðŸ“Š BATCH SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Total processed: {len(resume_files)}\")\n",
    "    print(f\"  âœ… Successful: {results['successful']}\")\n",
    "    print(f\"  âŒ Failed: {results['failed']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Process first 3 resumes from data folder (uncomment to run)\n",
    "# batch_results = batch_process_resumes(\"data\", limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf360baf",
   "metadata": {},
   "source": [
    "## Advanced Queries\n",
    "\n",
    "Some useful queries for searching the ATS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_skill(skill: str, db_path: str = \"resume_ats.db\"):\n",
    "    \"\"\"Search candidates by skill\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT DISTINCT c.id, c.name, c.email, c.total_years_experience\n",
    "        FROM candidates c\n",
    "        JOIN skills s ON c.id = s.candidate_id\n",
    "        WHERE s.skill LIKE ?\n",
    "        ORDER BY c.total_years_experience DESC\n",
    "    ''', (f'%{skill}%',))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\nðŸ” Candidates with skill: {skill}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if results:\n",
    "        for cid, name, email, exp in results:\n",
    "            print(f\"  â€¢ {name} ({exp or 'N/A'} years) - {email}\")\n",
    "    else:\n",
    "        print(f\"  No candidates found with skill: {skill}\")\n",
    "    \n",
    "    conn.close()\n",
    "    return results\n",
    "\n",
    "def search_by_experience(min_years: float, db_path: str = \"resume_ats.db\"):\n",
    "    \"\"\"Search candidates by minimum years of experience\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT id, name, email, total_years_experience\n",
    "        FROM candidates\n",
    "        WHERE total_years_experience >= ?\n",
    "        ORDER BY total_years_experience DESC\n",
    "    ''', (min_years,))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    print(f\"\\nðŸ” Candidates with {min_years}+ years of experience\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if results:\n",
    "        for cid, name, email, exp in results:\n",
    "            print(f\"  â€¢ {name} ({exp} years) - {email}\")\n",
    "    else:\n",
    "        print(f\"  No candidates found with {min_years}+ years\")\n",
    "    \n",
    "    conn.close()\n",
    "    return results\n",
    "\n",
    "# Example searches (uncomment to run after processing resumes)\n",
    "# search_by_skill(\"Python\")\n",
    "# search_by_experience(5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa965692",
   "metadata": {},
   "source": [
    "## Workflow Visualization\n",
    "\n",
    "The LangGraph workflow follows this structure:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   RESUME TRANSFORMER WORKFLOW                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                        â”‚   START      â”‚\n",
    "                        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                               â”‚\n",
    "                               â–¼\n",
    "                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                     â”‚  1. PARSE       â”‚\n",
    "                     â”‚  Extract Text   â”‚\n",
    "                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                     â”‚  2. EXTRACT     â”‚\n",
    "                     â”‚  LLM Analysis   â”‚\n",
    "                     â”‚  (GPT-4o-mini)  â”‚\n",
    "                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                     â”‚  3. VALIDATE    â”‚\n",
    "                     â”‚  & ENRICH       â”‚\n",
    "                     â”‚  â€¢ Clean Data   â”‚\n",
    "                     â”‚  â€¢ Calc Exp     â”‚\n",
    "                     â”‚  â€¢ Standardize  â”‚\n",
    "                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                     â”‚  4. STORE       â”‚\n",
    "                     â”‚  Insert to DB   â”‚\n",
    "                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                        â”‚    END       â”‚\n",
    "                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Features:\n",
    "- âœ… **Multi-format support**: PDF, DOCX, TXT\n",
    "- âœ… **LLM-powered extraction**: Uses GPT-4o-mini for intelligent parsing\n",
    "- âœ… **Data validation**: Cleans and normalizes data\n",
    "- âœ… **Experience calculation**: Automatically computes total years\n",
    "- âœ… **Skill standardization**: Maps to company taxonomy\n",
    "- âœ… **Database storage**: Structured SQL schema for ATS\n",
    "- âœ… **Batch processing**: Handle multiple resumes\n",
    "- âœ… **Search capabilities**: Query by skills, experience, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
